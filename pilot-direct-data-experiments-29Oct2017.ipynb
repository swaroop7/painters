{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "make_submission() generates predictions for the Kaggle Painter by Numbers competion\n",
    "using simple features (image size, aspect ratio and bits/pixel^2)\n",
    "author: Swaroop Krothapalli - extended code of small yello duck\n",
    "https://github.com/swaroop7/painters\n",
    "'''\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.stats import itemfreq\n",
    "from sklearn import neighbors, linear_model\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition, pipeline, metrics, grid_search\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.set_printoptions(precision=3, linewidth=100)\n",
    "random.seed(7)\n",
    "\n",
    "mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Pilot\\\\Artwork_for_Testing')\n",
    "\n",
    "\n",
    "def getEntropy(signal):\n",
    "    lensig=signal.size\n",
    "    symset=list(set(signal))\n",
    "    numsym=len(symset)\n",
    "    probabability_distribution=[np.size(signal[signal==i])/(1.0*lensig) for i in symset]\n",
    "    entropy=np.sum([p*np.log2(1.0/p) for p in probabability_distribution])\n",
    "    return entropy\n",
    "\n",
    "def calculateEntropyNeighbourhood(artwork, neighbourhood):\n",
    "    image = cv2.imread(artwork)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    colorIm=np.array(image)\n",
    "    grayIm=np.array(gray_image)\n",
    "    \n",
    "    N=neighbourhood\n",
    "    S=grayIm.shape\n",
    "    E=np.array(grayIm)\n",
    "    \n",
    "    for row in range(S[0]): \n",
    "            for col in range (S[1]): \n",
    "                    Lx=np.max([0,col-N]) \n",
    "                    Ux=np.min([S[1],col+N])\n",
    "                    Ly=np.max([0,row-N])\n",
    "                    Uy=np.min([S[0],row+N])\n",
    "                    # makes region 1-D\n",
    "                    region=grayIm[Ly:Uy,Lx:Ux].flatten()\n",
    "                    E[row,col]=getEntropy(region)\n",
    "    \n",
    "    average=np.mean(E)\n",
    "    return average\n",
    "\n",
    "def getDTM(artwork, neighbourhood):\n",
    "    image = cv2.imread(artwork)\n",
    "    image32f = np.float32(image)\n",
    "    mu    = cv2.blur(image32f,(neighbourhood,neighbourhood))\n",
    "    mu2   = cv2.blur(cv2.multiply(image32f,image32f), (neighbourhood,neighbourhood))\n",
    "    sigma = cv2.sqrt( mu2 - cv2.multiply(mu, mu) )\n",
    "    return np.mean(sigma)\n",
    "\n",
    "def get_image_info(test_info, dir):\n",
    "\tif dir == 'test':\n",
    "\t\timages = list(set(list(test_info.image1.unique()) + list(test_info.image2.unique())))\n",
    "\t\tinfo = pd.DataFrame(np.array(images).reshape((-1, 1)), columns = ['filename'])\n",
    "\n",
    "\telse:\n",
    "\t\tinfo = test_info\n",
    "\t\n",
    "\tinfo['pixelsx'] = np.nan\n",
    "\tinfo['pixelsy'] = np.nan\n",
    "\tinfo['size_bytes'] = np.nan\n",
    "\tinfo['entropy1'] = np.nan\n",
    "\tinfo['entropy5'] = np.nan\n",
    "\tinfo['entropy10'] = np.nan\n",
    "\tinfo['entropy15'] = np.nan\n",
    "\tinfo['dtm1'] = np.nan\n",
    "\tinfo['dtm5'] = np.nan\n",
    "\tinfo['dtm10'] = np.nan\n",
    "\tinfo['dtm15'] = np.nan\n",
    "\n",
    "\n",
    "\tj = 0\n",
    "\tfor i in info.index.values:\n",
    "\t\tj += 1        \n",
    "\t\ttry:\n",
    "\t\t\tfil = 'C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Pilot\\\\Artwork_for_Testing\\\\'+info.loc[i, 'filename']\n",
    "\t\t\tim = Image.open('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Pilot\\\\Artwork_for_Testing\\\\'+info.loc[i, 'filename'])\n",
    "\t\t\tinfo.loc[i, 'pixelsx'], info.loc[i, 'pixelsy'] = im.size\n",
    "\t\t\tinfo.loc[i, 'size_bytes'] = os.path.getsize(info.loc[i, 'filename'])\n",
    "\t\t\tinfo.loc[i, 'entropy1'] = calculateEntropyNeighbourhood(fil, 1)\n",
    "\t\t\tinfo.loc[i, 'entropy5'] = calculateEntropyNeighbourhood(fil, 5)\n",
    "\t\t\tinfo.loc[i, 'entropy10'] = calculateEntropyNeighbourhood(fil, 10)\n",
    "\t\t\tinfo.loc[i, 'entropy15'] = calculateEntropyNeighbourhood(fil, 15)\n",
    "\t\t\tinfo.loc[i, 'dtm1'] = getDTM(fil, 1)\n",
    "\t\t\tinfo.loc[i, 'dtm5'] = getDTM(fil, 5)\n",
    "\t\t\tinfo.loc[i, 'dtm10'] = getDTM(fil, 10)\n",
    "\t\t\tinfo.loc[i, 'dtm15'] = getDTM(fil, 15)\n",
    "\t\texcept:\n",
    "\t\t\tprint dir+'\\\\'+info.loc[i, 'filename']\n",
    "\t\tif (j%10 == 0):\n",
    "\t\t\tprint '',\n",
    "\tinfo=info.dropna()\n",
    "\tprint 'info shape',info.shape\n",
    "\treturn info.rename(columns={'filename' : 'new_filename'})\n",
    "\n",
    "def make_pairs(train_info):\n",
    "\tartists = train_info.artist.unique()\n",
    "\n",
    "\tn = train_info.groupby('artist').size()\n",
    "\tn = (2*n**2).sum() \n",
    "\tt = pd.DataFrame(np.zeros((n, 4)), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "\ti = 0\n",
    "\tj = 0\n",
    "\tfor m in artists:\n",
    "\t\t\n",
    "\t\ta = train_info[train_info.artist==m][['artist', 'new_filename']].values\n",
    "\t\tuse = train_info[train_info.artist != m].index.values\n",
    "\t\tnp.random.shuffle(use)\n",
    "\t\tnm = np.mean([a.shape[0]**2, train_info[train_info.artist != m].shape[0] ])\n",
    "\n",
    "\t\tuse = use[0:nm]\n",
    "\t\tb = train_info[train_info.artist!=m][['artist', 'new_filename']].ix[use, :].values\n",
    "\t\ta2 = pd.DataFrame(np.concatenate([np.repeat(a[:, 0], a.shape[0]).reshape((-1,1)), np.repeat(a[:, 1], a.shape[0]).reshape((-1,1)), np.tile(a, (a.shape[0], 1))], axis=1), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "\t\ta2 = a2.loc[0:nm, :]\n",
    "\t\tb2 = pd.DataFrame(np.concatenate([np.tile(a, (a.shape[0], 1))[0:b.shape[0], :], b], axis=1), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "\t\tt.iloc[i:i+a2.shape[0], :] = a2.values\n",
    "\t\tt.iloc[i+a2.shape[0]:i+a2.shape[0]+b2.shape[0], :] = b2.values\n",
    "\t\ti += a2.shape[0] +b2.shape[0]\n",
    "\t\tj += 1\n",
    "\t\n",
    "\tt = t[~t.image2.isin([np.nan, 0])]\n",
    "\treturn t.drop_duplicates(subset=['artist1', 'artist2','image1', 'image2'], keep=False)\n",
    "\n",
    "\n",
    "def prep_data(input, split):\n",
    "\tinfo = input[0]\n",
    "\tdata = input[1]\n",
    "\t\n",
    "\tif split=='cv':\n",
    "\t\tartists = info.artist\n",
    "\t\tnp.random.shuffle(artists)\n",
    "\t\tinfo = get_image_info(info, 'train')\n",
    "\t\tinfo['bytes_per_pixel'] = 1.0*info['size_bytes']/(info['pixelsx']*info['pixelsy'])\n",
    "\t\tinfo['aspect_ratio'] = 1.0*info['pixelsx']/info['pixelsy']\t\n",
    "\t\tprint 'hi',info[info.artist.isin(artists)].shape\n",
    "\t\tprint info.columns\n",
    "\t\tinfo['artist'] = info['artist'].map({'hudsonriver': 1, 'impressionist': 0})\n",
    "\t\ty_train = info['artist']\n",
    "\t\tx_train = info.drop(['artist', 'new_filename'], axis=1) \n",
    "\t\tprint x_train.columns\n",
    "\t\tprint y_train\n",
    "\t\tprint x_train\n",
    "\n",
    "\tif split=='test':\n",
    "\n",
    "\t\tinfo = get_image_info(data, 'test')\n",
    "\t\tinfo['bytes_per_pixel'] = 1.0*info['size_bytes']/(info['pixelsx']*info['pixelsy'])\n",
    "\t\tinfo['aspect_ratio'] = 1.0*info['pixelsx']/info['pixelsy']\t\n",
    "\t\tdata['in_train'] = False\n",
    "\t\tif 'artist1' in data.columns:\n",
    "\t\t\tdata['sameArtist'] = data['artist1'] == data['artist2']\n",
    "\n",
    "\n",
    "\tif split=='cv':\n",
    "\t\treturn x_train, y_train, x_train, y_train  \n",
    " \tif split=='test':\n",
    "\t\treturn x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepping training and cv data\n",
      "  train\\Monet1880b.jpg\n",
      "train\\Renoir-1873.jpg\n",
      "train\\Renoir-1874.jpg\n",
      "train\\Renoir-1880.jpg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "train_info = pd.read_csv('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\pilot_medium.csv')\n",
    "print 'prepping training and cv data'\n",
    "x_train, y_train, x_cv, y_cv = prep_data([train_info, None], 'cv')\n",
    "\n",
    "print x_train.shape\n",
    "\n",
    "\n",
    "print (time.time() - start_time)/60 , \"minutes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = np.loadtxt('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Pilot\\\\Artwork_for_Testing\\\\x_train_pilot.txt')\n",
    "y_train = np.loadtxt('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Pilot\\\\Artwork_for_Testing\\\\y_train_pilot.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c6db06655b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ent5_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy5'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ent10_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy10'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ent15_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy15'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ent10_5'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy10'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy5'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ent15_5'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy15'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy5'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "x_train['ent5_1'] = x_train['entropy5'] - x_train['entropy1']\n",
    "x_train['ent10_1'] = x_train['entropy10'] - x_train['entropy1']\n",
    "x_train['ent15_1'] = x_train['entropy15'] - x_train['entropy1']\n",
    "x_train['ent10_5'] = x_train['entropy10'] - x_train['entropy5']\n",
    "x_train['ent15_5'] = x_train['entropy15'] - x_train['entropy5']\n",
    "x_train['ent15_10'] = x_train['entropy15'] - x_train['entropy10']\n",
    "\n",
    "\n",
    "x_train['ent5d1'] = x_train['entropy5'] / x_train['entropy1']\n",
    "x_train['ent10d1'] = x_train['entropy10'] / x_train['entropy1']\n",
    "x_train['ent15d1'] = x_train['entropy15'] / x_train['entropy1']\n",
    "x_train['ent10d5'] = x_train['entropy10'] / x_train['entropy5']\n",
    "x_train['ent15d5'] = x_train['entropy15'] / x_train['entropy5']\n",
    "x_train['ent15d10'] = x_train['entropy15'] / x_train['entropy10']\n",
    "\n",
    "x_train['ent1d5'] = x_train['entropy1'] / x_train['entropy5']\n",
    "x_train['ent1d10'] = x_train['entropy1'] / x_train['entropy10']\n",
    "x_train['ent1d15'] = x_train['entropy1'] / x_train['entropy15']\n",
    "x_train['ent5d10'] = x_train['entropy5'] / x_train['entropy10']\n",
    "x_train['ent5d15'] = x_train['entropy5'] / x_train['entropy15']\n",
    "x_train['ent10d15'] = x_train['entropy10'] / x_train['entropy15']\n",
    "\n",
    "\n",
    "x_train['dtm10_5'] = x_train['dtm10'] - x_train['dtm5']\n",
    "x_train['dtm15_5'] = x_train['dtm15'] - x_train['dtm5']\n",
    "x_train['dtm15_10'] = x_train['dtm15'] - x_train['dtm10']\n",
    "\n",
    "x_train['dtm10d5'] = x_train['dtm10'] / x_train['dtm5']\n",
    "x_train['dtm15d5'] = x_train['dtm15'] / x_train['dtm5']\n",
    "x_train['dtm15d10'] = x_train['dtm15'] / x_train['dtm10']\n",
    "\n",
    "x_train['dtm5d10'] = x_train['dtm5'] / x_train['dtm10']\n",
    "x_train['dtm5d15'] = x_train['dtm5'] / x_train['dtm15']\n",
    "x_train['dtm10d15'] = x_train['dtm10'] / x_train['dtm15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_results(clf, y_test, y_pred, y_pred_prob):\n",
    "    print 'ROC - ',roc_auc_score(y_test, y_pred_prob)\n",
    "    print 'Confusion Matrix - ', confusion_matrix(y_test, y_pred)\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    print classification_report(y_test, y_pred, target_names=target_names)\n",
    "    \n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "def report(grid_scores, n_top):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.4f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31L, 22L) (31L,)\n",
      "(24L, 22L) (7L, 22L)\n",
      "[[  0.  13.]\n",
      " [  1.  11.]]\n",
      "[[ 0.  4.]\n",
      " [ 1.  3.]]\n",
      "[[ 0.  3.]\n",
      " [ 1.  2.]]\n",
      "[[  0.  17.]\n",
      " [  1.  14.]]\n",
      "[[  0.  10.]\n",
      " [  1.   9.]]\n",
      "[ 1.  1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape, y_train.shape\n",
    "training, testing, y_training, y_testing = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "actual_training, validation, y_actual_training, y_validation = train_test_split(training, y_training, test_size=0.2, stratify=y_training, random_state=42)\n",
    "print training.shape, testing.shape\n",
    "#print y_training, y_testing\n",
    "print itemfreq(y_training)\n",
    "print itemfreq(y_testing)\n",
    "print itemfreq(y_validation)\n",
    "print itemfreq(y_train)\n",
    "print itemfreq(y_actual_training)\n",
    "\n",
    "print y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    8.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=9, max_features=5,\n",
      "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "            min_samples_leaf=5, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
      "            oob_score=True, random_state=9001, verbose=0, warm_start=False)\n",
      " Test Accuracy: 57.14%\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.6250)\n",
      "Parameters: {'oob_score': True, 'min_samples_leaf': 5, 'n_estimators': 50, 'max_features': 5, 'random_state': 9001, 'min_samples_split': 2, 'max_depth': 9, 'class_weight': 'balanced'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.5417)\n",
      "Parameters: {'oob_score': True, 'min_samples_leaf': 10, 'n_estimators': 100, 'max_features': None, 'random_state': 9001, 'min_samples_split': 15, 'max_depth': 6, 'class_weight': 'balanced'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.5000)\n",
      "Parameters: {'oob_score': True, 'min_samples_leaf': 10, 'n_estimators': 200, 'max_features': 15, 'random_state': 9001, 'min_samples_split': 15, 'max_depth': 20, 'class_weight': 'balanced'}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.5000)\n",
      "Parameters: {'oob_score': True, 'min_samples_leaf': 2, 'n_estimators': 150, 'max_features': 10, 'random_state': 9001, 'min_samples_split': 5, 'max_depth': 20, 'class_weight': 'balanced'}\n",
      "\n",
      "ROC -  0.666666666667\n",
      "Confusion Matrix -  [[1 3]\n",
      " [0 3]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.25      0.40         4\n",
      "    class 1       0.50      1.00      0.67         3\n",
      "\n",
      "avg / total       0.79      0.57      0.51         7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swaroop\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "random.seed(9001)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_dist = {\"max_depth\": [3, 6, 9,12,20],\n",
    "              \"max_features\": [5,10,15,20,None],\n",
    "              \"min_samples_split\": [2, 5, 10, 15,20,50,100],\n",
    "              \"min_samples_leaf\": [ 1, 2, 5, 10],\n",
    "              \"class_weight\": ['balanced'],\n",
    "              \"n_estimators\": [10,20,50,100, 150, 200],\n",
    "               \"oob_score\": [True],\n",
    "                \"random_state\": [9001]}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, cv=5, scoring='accuracy',  n_jobs=-1, verbose=1)\n",
    "                                   #n_iter=n_iter_search)\n",
    "\n",
    "random_search.fit(training, y_training)\n",
    "\n",
    "\n",
    "best_est = random_search.best_estimator_\n",
    "\n",
    "print best_est\n",
    "\n",
    "print(\" Test Accuracy: %.2f%%\" % (accuracy_score(y_testing, best_est.predict(testing)) * 100.0))\n",
    "report(random_search.grid_scores_, 4)\n",
    "\n",
    "print_results(best_est, y_testing, best_est.predict(testing), best_est.predict_proba(testing)[:,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=10, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=1.0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      " Test Accuracy: 57.14%\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.5833)\n",
      "Parameters: {'n_estimators': 10, 'subsample': 0.8, 'reg_alpha': 1.0, 'colsample_bytree': 0.9, 'max_depth': 3}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.5000)\n",
      "Parameters: {'n_estimators': 22, 'subsample': 0.6, 'reg_alpha': 0, 'colsample_bytree': 1.0, 'max_depth': 3}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.5000)\n",
      "Parameters: {'n_estimators': 100, 'subsample': 0.9, 'reg_alpha': 1.0, 'colsample_bytree': 0.5, 'max_depth': 2}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.5000)\n",
      "Parameters: {'n_estimators': 100, 'subsample': 0.9, 'reg_alpha': 0.1, 'colsample_bytree': 0.9, 'max_depth': 3}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.5000)\n",
      "Parameters: {'n_estimators': 3, 'subsample': 0.4, 'reg_alpha': 1.0, 'colsample_bytree': 0.7, 'max_depth': 3}\n",
      "\n",
      "ROC -  0.666666666667\n",
      "Confusion Matrix -  [[2 2]\n",
      " [1 2]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.67      0.50      0.57         4\n",
      "    class 1       0.50      0.67      0.57         3\n",
      "\n",
      "avg / total       0.60      0.57      0.57         7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    2.8s finished\n",
      "C:\\Users\\swaroop\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#XGBoost Classifier with GridSearch\n",
    "\n",
    "params={\n",
    "    'max_depth': [2,3,4], #[3,4,5,6,7,8,9], # 5 is good \n",
    "    'subsample': [0.4,0.5,0.6,0.7,0.8,0.9,1.0], #[0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "    'colsample_bytree': [0.5,0.6,0.7,0.8,0.9,1.0], #[0.5,0.6,0.7,0.8],\n",
    "    'n_estimators': [3, 10, 22, 25, 40, 100], #[1000,2000,3000]\n",
    "    #\"n_estimators\": st.randint(3, 40),\n",
    "    'reg_alpha': [0, 0.03, 0.1, 0.5, 1.0] #[0.01, 0.02, 0.03, 0.04]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "rs = RandomizedSearchCV(xgb_clf,\n",
    "                  params,\n",
    "                  cv=5,\n",
    "                  scoring=\"accuracy\",\n",
    "                  n_jobs=-1,\n",
    "                  verbose=1)\n",
    "rs.fit(training, y_training)\n",
    "best_est = rs.best_estimator_\n",
    "print(best_est)\n",
    "\n",
    "\n",
    "print(\" Test Accuracy: %.2f%%\" % (accuracy_score(y_testing, best_est.predict(testing)) * 100.0))\n",
    "\n",
    "report(rs.grid_scores_, 5)\n",
    "\n",
    "#xgb_pred = best_est.predict(score_df)\n",
    "print_results(best_est, y_testing, best_est.predict(testing), best_est.predict_proba(testing)[:,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('logistic', LogisticRegression(C=0.001, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=999,\n",
      "          solver='liblinear', tol=0.01, verbose=0, warm_start=False))])\n",
      " Test Accuracy: 57.14%\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.5417)\n",
      "Parameters: {'logistic__C': 0.001, 'logistic__random_state': 999, 'logistic__tol': 0.01, 'logistic__class_weight': 'balanced'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.5417)\n",
      "Parameters: {'logistic__C': 0.001, 'logistic__random_state': 999, 'logistic__tol': 0.001, 'logistic__class_weight': 'balanced'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.5417)\n",
      "Parameters: {'logistic__C': 0.001, 'logistic__random_state': 999, 'logistic__tol': 0.0001, 'logistic__class_weight': 'balanced'}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.5417)\n",
      "Parameters: {'logistic__C': 0.01, 'logistic__random_state': 999, 'logistic__tol': 0.01, 'logistic__class_weight': 'balanced'}\n",
      "\n",
      "ROC -  0.583333333333\n",
      "Confusion Matrix -  [[4 0]\n",
      " [3 0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.57      1.00      0.73         4\n",
      "    class 1       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.33      0.57      0.42         7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "logistic = LogisticRegression(random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps=[('logistic', logistic)])\n",
    "\n",
    "a = 'ovr'\n",
    "estimator = GridSearchCV(pipe,\n",
    "                         dict(\n",
    "                              #pca__n_components=n_components,\n",
    "                              logistic__C=[0.001, 0.01, 0.1, 1, 10, 100], \n",
    "                              logistic__random_state=[999],\n",
    "                             # logistic__intercept_scaling=[1.0],\n",
    "                              logistic__tol=[0.1,0.01, 0.001, 0.0001],\n",
    "                              #logistic__dual=[True],\n",
    "                              #logistic__multi_class=['ovr'],\n",
    "                            logistic__class_weight =['balanced']))\n",
    "estimator.fit(training, y_training)\n",
    "\n",
    "best_model = estimator.best_estimator_\n",
    "print best_model    \n",
    "    \n",
    "print(\" Test Accuracy: %.2f%%\" % (accuracy_score(y_testing, best_model.predict(testing)) * 100.0))\n",
    "\n",
    "report(estimator.grid_scores_, 4)\n",
    "\n",
    "print_results(best_model, y_testing, best_model.predict(testing), best_model.predict_proba(testing)[:,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swaroop\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:552: Warning: The least populated class in y has only 9 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=10.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.789\n",
      "Best parameters set:\n",
      "\tsvm__C: 1\n",
      "Pipeline(steps=[('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=1, shrinking=True, tol=0.001,\n",
      "  verbose=False))])\n",
      " Test Accuracy: 71.43%\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.7895)\n",
      "Parameters: {'svm__C': 1}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.7368)\n",
      "Parameters: {'svm__C': 9}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.7368)\n",
      "Parameters: {'svm__C': 10}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.7368)\n",
      "Parameters: {'svm__C': 12}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.7368)\n",
      "Parameters: {'svm__C': 100}\n",
      "\n",
      "ROC -  0.75\n",
      "Confusion Matrix -  [[2 2]\n",
      " [0 3]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.50      0.67         4\n",
      "    class 1       0.60      1.00      0.75         3\n",
      "\n",
      "avg / total       0.83      0.71      0.70         7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVC with SVD\n",
    "    \n",
    "scl = StandardScaler()\n",
    "    \n",
    "svm_model = SVC(random_state=1, probability=True)\n",
    "    \n",
    "clf = pipeline.Pipeline([('scl', scl), ('svm', svm_model)])\n",
    "    \n",
    "    \n",
    "param_grid = {'svm__C': [0.1, 1, 9,10, 12, 100]}\n",
    "              #, 'svm__decision_function_shape': ['ovr']}\n",
    "    \n",
    "model = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring='accuracy', verbose=0, n_jobs=-1, iid=True, refit=True, cv=10)\n",
    "                                     \n",
    "\n",
    "model.fit(actual_training, y_actual_training)\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    \n",
    "best_model = model.best_estimator_\n",
    "    \n",
    "print best_model\n",
    "\n",
    "print(\" Test Accuracy: %.2f%%\" % (accuracy_score(y_testing, best_model.predict(testing)) * 100.0))\n",
    "\n",
    "report(model.grid_scores_, 5)\n",
    "\n",
    "print_results(best_model, y_testing, best_model.predict(testing), best_model.predict_proba(testing)[:,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
