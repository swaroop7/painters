{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "make_submission() generates predictions for the Kaggle Painter by Numbers competion\n",
    "using simple features (image size, aspect ratio and bits/pixel^2)\n",
    "author: Swaroop Krothapalli - extended code of small yello duck\n",
    "https://github.com/swaroop7/painters\n",
    "'''\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cross_validation import KFold\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.stats import itemfreq\n",
    "from sklearn import neighbors, linear_model\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Wyeths')\n",
    "\n",
    "def getEntropy(signal):\n",
    "    lensig=signal.size\n",
    "    symset=list(set(signal))\n",
    "    numsym=len(symset)\n",
    "    probabability_distribution=[np.size(signal[signal==i])/(1.0*lensig) for i in symset]\n",
    "    entropy=np.sum([p*np.log2(1.0/p) for p in probabability_distribution])\n",
    "    return entropy\n",
    "\n",
    "def calculateEntropyNeighbourhood(artwork, neighbourhood):\n",
    "    image = cv2.imread(artwork)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    colorIm=np.array(image)\n",
    "    grayIm=np.array(gray_image)\n",
    "    \n",
    "    N=neighbourhood\n",
    "    S=grayIm.shape\n",
    "    E=np.array(grayIm)\n",
    "    \n",
    "    for row in range(S[0]): \n",
    "            for col in range (S[1]): \n",
    "                    Lx=np.max([0,col-N]) \n",
    "                    Ux=np.min([S[1],col+N])\n",
    "                    Ly=np.max([0,row-N])\n",
    "                    Uy=np.min([S[0],row+N])\n",
    "                    # makes region 1-D\n",
    "                    region=grayIm[Ly:Uy,Lx:Ux].flatten()\n",
    "                    E[row,col]=getEntropy(region)\n",
    "    \n",
    "    average=np.mean(E)\n",
    "    return average\n",
    "\n",
    "def getDTM(artwork):\n",
    "    image = cv2.imread(artwork)\n",
    "    image32f = np.float32(image)\n",
    "    mu    = cv2.blur(image32f,(5,5))\n",
    "    mu2   = cv2.blur(cv2.multiply(image32f,image32f), (5,5))\n",
    "    sigma = cv2.sqrt( mu2 - cv2.multiply(mu, mu) )\n",
    "    return np.mean(sigma)\n",
    "\n",
    "#image_info_test = get_image_info(test_info, 'test')\n",
    "def get_image_info(test_info, dir):\n",
    "\tif dir == 'test':\n",
    "\t\timages = list(set(list(test_info.image1.unique()) + list(test_info.image2.unique())))\n",
    "\t\tinfo = pd.DataFrame(np.array(images).reshape((-1, 1)), columns = ['filename'])\n",
    "\t\t#print info\n",
    "\telse:\n",
    "\t\tinfo = test_info\n",
    "\t\n",
    "\tinfo['pixelsx'] = np.nan\n",
    "\tinfo['pixelsy'] = np.nan\n",
    "\tinfo['size_bytes'] = np.nan\n",
    "# \tinfo['entropy1'] = np.nan\n",
    "# \tinfo['entropy5'] = np.nan\n",
    "# \tinfo['entropy10'] = np.nan\n",
    "# \tinfo['entropy15'] = np.nan\n",
    "# \tinfo['entropy20'] = np.nan\n",
    "\tinfo['dtm'] = np.nan\n",
    "\n",
    "    \n",
    "\t\n",
    "\tj = 0\n",
    "\tfor i in info.index.values:\n",
    "\t\tj += 1        \n",
    "\t\ttry:\n",
    "\t\t\t#print i\n",
    "\t\t\t#fil = 'C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Wyeths\\\\'+dir+'\\\\'+info.loc[i, 'filename']\n",
    "\t\t\tfil = 'C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Wyeths\\\\'+info.loc[i, 'filename']\n",
    "\t\t\t#print fil\n",
    "\t\t\tim = Image.open('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Wyeths\\\\'+info.loc[i, 'filename'])\n",
    "\t\t\t#print im\n",
    "\t\t\t#print im.size\n",
    "\t\t\tinfo.loc[i, 'pixelsx'], info.loc[i, 'pixelsy'] = im.size\n",
    "\t\t\t#im = cv2.imread(dir+'/'+info.loc[i, 'new_filename'])\n",
    "\t\t\t#info.loc[i, 'pixelsx'], info.loc[i, 'pixelsy'] = im.shape[0:2]\n",
    "\t\t\tinfo.loc[i, 'size_bytes'] = os.path.getsize(info.loc[i, 'filename'])\n",
    "\t\t\t#info.loc[i, 'entropy'] = calculateEntropyNeighbourhood(fil, 1)\n",
    "\t\t\t#print calculateEntropyNeighbourhood(fil, 1)\n",
    "\t\t\tinfo.loc[i, 'dtm'] = getDTM(fil)\n",
    "# \t\t\tinfo['entropy1'] = calculateEntropyNeighbourhood(fil, 1)\n",
    "# \t\t\tinfo['entropy5'] = calculateEntropyNeighbourhood(fil, 5)\n",
    "# \t\t\tinfo['entropy10'] = calculateEntropyNeighbourhood(fil, 10)\n",
    "# \t\t\tinfo['entropy15'] = calculateEntropyNeighbourhood(fil, 15)\n",
    "# \t\t\tinfo['entropy20'] = calculateEntropyNeighbourhood(fil, 20)\n",
    "\n",
    "\t\texcept:\n",
    "\t\t\tprint dir+'\\\\'+info.loc[i, 'filename']\n",
    "\t\tif (j%10 == 0):\n",
    "\t\t\tprint '',\n",
    "\tinfo=info.dropna()\n",
    "\tprint 'info shape',info.shape\n",
    "\treturn info.rename(columns={'filename' : 'new_filename'})\n",
    "\n",
    "#t = make_pairs(train_info)\t\n",
    "def make_pairs(train_info):\n",
    "\tprint \"make pairs train info shape\",train_info.shape\n",
    "\tartists = train_info.artist.unique()\n",
    "\n",
    "\tn = train_info.groupby('artist').size()\n",
    "\tn = (2*n**2).sum() \n",
    "\tt = pd.DataFrame(np.zeros((n, 4)), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "\ti = 0\n",
    "\tj = 0\n",
    "\tfor m in artists:\n",
    "\t\t\n",
    "\t\ta = train_info[train_info.artist==m][['artist', 'new_filename']].values\n",
    "\t\tuse = train_info[train_info.artist != m].index.values\n",
    "\t\tprint \"a and use shapes\", a.shape, use.shape\n",
    "\t\tnp.random.shuffle(use)\n",
    "\t\t#print a.shape, use.shape\n",
    "\t\tnm = np.mean([a.shape[0]**2, train_info[train_info.artist != m].shape[0] ])\n",
    "\t\tprint nm\n",
    "\t\tuse = use[0:nm]\n",
    "\t\tprint \"use.shape\",use.shape\n",
    "\t\tb = train_info[train_info.artist!=m][['artist', 'new_filename']].ix[use, :].values\n",
    "\t\t#print nm, use.shape, b.shape\n",
    "\t\ta2 = pd.DataFrame(np.concatenate([np.repeat(a[:, 0], a.shape[0]).reshape((-1,1)), np.repeat(a[:, 1], a.shape[0]).reshape((-1,1)), np.tile(a, (a.shape[0], 1))], axis=1), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "\t\ta2 = a2.loc[0:nm, :]\n",
    "\t\tb2 = pd.DataFrame(np.concatenate([np.tile(a, (a.shape[0], 1))[0:b.shape[0], :], b], axis=1), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "\t\tprint j, i, a2.shape[0], b2.shape[0]\n",
    "\t\t#print b2\n",
    "\t\tt.iloc[i:i+a2.shape[0], :] = a2.values\n",
    "\t\tt.iloc[i+a2.shape[0]:i+a2.shape[0]+b2.shape[0], :] = b2.values\n",
    "\t\ti += a2.shape[0] +b2.shape[0]\n",
    "\t\tj += 1\n",
    "\t\n",
    "\tt = t[~t.image2.isin([np.nan, 0])]\n",
    "\tprint t.shape, t[t.image1 > t.image2].shape\n",
    "\tprint t.columns.values\n",
    "\t#print t\n",
    "\tprint \"hi1\",t.drop_duplicates(subset=['artist1', 'artist2','image1', 'image2'], keep=False).shape\n",
    "\t#return t[t.image1 > t.image2]\t\n",
    "\treturn t.drop_duplicates(subset=['artist1', 'artist2','image1', 'image2'], keep=False)\n",
    "\n",
    "\n",
    "#x_train, y_train, x_cv, y_cv = prep_data([train_info, None], 'cv')\t\n",
    "#x_test, y_test = prep_data([None, submission_info], 'test')\t\n",
    "def prep_data(input, split):\n",
    "\tinfo = input[0]\n",
    "\tdata = input[1]\n",
    "\t\n",
    "\tif split=='cv':\n",
    "\t\t#artists = info.artist.unique()\n",
    "\t\tartists = info.artist\n",
    "\t\t#print artists\n",
    "\t\t#print 'hi', artists\n",
    "\t\tnp.random.shuffle(artists)\n",
    "\t\t\n",
    "\t\tinfo = get_image_info(info, 'train')\n",
    "\t\tinfo['bytes_per_pixel'] = 1.0*info['size_bytes']/(info['pixelsx']*info['pixelsy'])\n",
    "\t\tinfo['aspect_ratio'] = 1.0*info['pixelsx']/info['pixelsy']\n",
    "\t\t#train_artists = artists[0:int(0.8*len(artists))]\n",
    "\t\t#test_artists = artists[int(0.8*len(artists)):]\n",
    "\t\t#print artists\n",
    "\t\tprint 'hi',info[info.artist.isin(artists)].shape\n",
    "\t\ttrain = make_pairs(info[info.artist.isin(artists)])\n",
    "\t\t#test = make_pairs(info[info.artist.isin(test_artists)])\n",
    "\t\t#print train.shape\n",
    "\t\ttrain['in_train'] = True\n",
    "\t\t#test['in_train'] = True\n",
    "\t\tdata = train\n",
    "\t\tdata['sameArtist'] = data['artist1'] == data['artist2']\n",
    "\t\t\n",
    "\tif split=='test':\n",
    "\n",
    "\t\tinfo = get_image_info(data, 'test')\n",
    "\t\tinfo['bytes_per_pixel'] = 1.0*info['size_bytes']/(info['pixelsx']*info['pixelsy'])\n",
    "\t\tinfo['aspect_ratio'] = 1.0*info['pixelsx']/info['pixelsy']\t\n",
    "\t\t\n",
    "\t\tdata['in_train'] = False\n",
    "\t\n",
    "\t\tif 'artist1' in data.columns:\n",
    "\t\t\tdata['sameArtist'] = data['artist1'] == data['artist2']\n",
    "\n",
    "\t\n",
    "\tdata2 = pd.merge(data, info[['new_filename', 'pixelsx', 'pixelsy', 'size_bytes', 'bytes_per_pixel', 'aspect_ratio']], how='left', left_on='image1', right_on='new_filename')\n",
    "\tdata2.drop('new_filename', 1, inplace=True)\n",
    "\t\n",
    "\tdata2 = pd.merge(data2, info[['new_filename', 'pixelsx', 'pixelsy', 'size_bytes', 'bytes_per_pixel', 'aspect_ratio']], how='left', left_on='image2', right_on='new_filename')\n",
    "\tdata2.drop('new_filename', 1, inplace=True)\n",
    "\t\n",
    "\tx_train = data2[data2.in_train==True][['pixelsx_x', 'pixelsy_x', 'size_bytes_x', 'bytes_per_pixel_x', 'aspect_ratio_x', 'pixelsx_y', 'pixelsy_y', 'size_bytes_y', 'bytes_per_pixel_y', 'aspect_ratio_y']].values\n",
    "\tx_test = data2[data2.in_train==False][['pixelsx_x', 'pixelsy_x', 'size_bytes_x', 'bytes_per_pixel_x', 'aspect_ratio_x', 'pixelsx_y', 'pixelsy_y', 'size_bytes_y', 'bytes_per_pixel_y', 'aspect_ratio_y']].values\n",
    "\t\n",
    "\t\n",
    "\tif 'artist1' in data.columns: \n",
    "\t\ty_train = data2[data2.in_train==True]['sameArtist'].values\n",
    "\t\ty_test = data2[data2.in_train==False]['sameArtist'].values\n",
    "\telse:\n",
    "\t\ty_test = None\t\n",
    "\t\n",
    "\tif split=='cv':\t\t\n",
    "\t\treturn x_train, y_train, x_train, y_train  \n",
    " \tif split=='test':\n",
    "\t\treturn x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepping training and cv data\n",
      " train\\Distant Thunder\n",
      "  train\\John Andres House\n",
      "  train\\The Master Bedroom\n",
      "train\\Wind from the Sea\n",
      " info shape (50, 9)\n",
      "hi (50, 11)\n",
      "make pairs train info shape (50, 11)\n",
      "a and use shapes (28L, 2L) (22L,)\n",
      "403.0\n",
      "use.shape (22L,)\n",
      "0 0 404 22\n",
      "a and use shapes (22L, 2L) (28L,)\n",
      "256.0\n",
      "use.shape (28L,)\n",
      "1 426 257 28\n",
      "(711, 4) (203, 4)\n",
      "['artist1' 'image1' 'artist2' 'image2']\n",
      "hi1 (711, 4)\n",
      "(711L, 10L)\n",
      "0.0693833311399 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swaroop\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:151: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "train_info = pd.read_csv('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\wyeths_medium.csv')\n",
    "#submission_info = pd.read_csv('submission_info.csv')\n",
    "print 'prepping training and cv data'\n",
    "x_train, y_train, x_cv, y_cv = prep_data([train_info, None], 'cv')\n",
    "\n",
    "print x_train.shape\n",
    "#np.savetxt('x_train_wyeth.txt', x_train, fmt = '%1.3f' )\n",
    "#np.savetxt('y_train_wyeth.txt', y_train, fmt = '%1.3f' )\n",
    "\n",
    "print (time.time() - start_time)/60 , \"minutes\"\n",
    "\n",
    "#print 'prepping test data'\n",
    "#x_test, y_test = prep_data([None, submission_info], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_classifier(x_train, y_train, x_cv, y_cv):    \n",
    "    clf1 = SVC(kernel = 'sigmoid', class_weight  = 'balanced', probability=True, tol = 0.01)\n",
    "    clf2 = SVC(kernel = 'rbf', class_weight  = 'balanced', probability=True, tol = 0.01)\n",
    "    clf3 = RandomForestClassifier(n_estimators=10, n_jobs = -1)\n",
    "    clf4 = GaussianNB()\n",
    "    clf5 = BernoulliNB() \n",
    "    clf6 = LinearSVC(class_weight = 'balanced', dual = False)\n",
    "    clf7 = SVC(kernel = 'poly', class_weight  = 'balanced', probability=True, tol = 0.1, degree=2, C=1.0)\n",
    "    clf8 = neighbors.KNeighborsClassifier()\n",
    "    clf9 = linear_model.LogisticRegression()\n",
    "    clf10 = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05) #objective='multi:softprob'\n",
    "    print 'starting fit'\n",
    "    \n",
    "    print x_train.shape, y_train.shape\n",
    "    training, testing, y_training, y_testing = train_test_split(x_train, y_train, test_size=0.3)\n",
    "    \n",
    "    print training.shape, testing.shape\n",
    "    #print y_training, y_testing\n",
    "    print itemfreq(y_training)\n",
    "    print itemfreq(y_testing)\n",
    "    \n",
    "    clf1.fit(training, y_training)\n",
    "    clf2.fit(training, y_training)\n",
    "    clf3.fit(training, y_training)\n",
    "    clf4.fit(training, y_training)\n",
    "    clf5.fit(training, y_training)\n",
    "    clf6.fit(training, y_training)\n",
    "#     clf7.fit(training, y_training) \n",
    "    clf8.fit(training, y_training) \n",
    "    clf9.fit(training, y_training)\n",
    "    clf10.fit(training, y_training)\n",
    "\n",
    "    print 'SVM - Sigmoid Kernel'\n",
    "    print_results(clf1, y_testing, clf1.predict(testing), clf1.predict_proba(testing)[:,1] )\n",
    "    print 'SVM - rbf Kernel'\n",
    "    print_results(clf2, y_testing, clf2.predict(testing), clf2.predict_proba(testing)[:,1] )\n",
    "    print 'Random Forest'\n",
    "    print_results(clf3, y_testing, clf3.predict(testing), clf3.predict_proba(testing)[:,1] )\n",
    "    print 'Gaussian NB'\n",
    "    print_results(clf4, y_testing, clf4.predict(testing), clf4.predict_proba(testing)[:,1] )\n",
    "    print 'Bernoulli NB'\n",
    "    print_results(clf5, y_testing, clf5.predict(testing), clf5.predict_proba(testing)[:,1] )\n",
    "    print 'SVM Linear Kernel'    \n",
    "    prob_pos = clf6.decision_function(testing)\n",
    "    prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "    print_results(clf6, y_testing, clf6.predict(testing), prob_pos )\n",
    "#     print 'SVM Polynomial Kernel'\n",
    "#     print_results(clf7, y_testing, clf7.predict(testing), clf7.predict_proba(testing)[:,1] )\n",
    "    print 'Nearest Neighbors'\n",
    "    print_results(clf8, y_testing, clf8.predict(testing), clf8.predict_proba(testing)[:,1] )\n",
    "    print 'Logistic Regression'\n",
    "    print_results(clf9, y_testing, clf9.predict(testing), clf9.predict_proba(testing)[:,1] )\n",
    "    print 'XG Boost'\n",
    "    print_results(clf10, y_testing, clf10.predict(testing), clf10.predict_proba(testing)[:,1] )\n",
    "\n",
    "\n",
    "def print_results(clf, y_test, y_pred, y_pred_prob):\n",
    "    #y_pred_prob = clf.predict_proba(y_test)[:,1]\n",
    "    #y_pred = clf.predict(y_test)\n",
    "    print 'ROC - ',roc_auc_score(y_test, y_pred_prob)\n",
    "    print 'Confusion Matrix - ', confusion_matrix(y_test, y_pred)\n",
    "    #print 'Precision - ',precision_score(y_test, y_pred ),'Recall - ',recall_score(y_test, y_pred),'F1- Score',f1_score(y_test, y_pred),'\\n'\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    print classification_report(y_test, y_pred, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting fit\n",
      "(711L, 10L) (711L,)\n",
      "(497L, 10L) (214L, 10L)\n",
      "[[  0  36]\n",
      " [  1 461]]\n",
      "[[  0  14]\n",
      " [  1 200]]\n",
      "SVM - Sigmoid Kernel\n",
      "ROC -  0.5\n",
      "Confusion Matrix -  [[  0  14]\n",
      " [  0 200]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.00      0.00      0.00        14\n",
      "    class 1       0.93      1.00      0.97       200\n",
      "\n",
      "avg / total       0.87      0.93      0.90       214\n",
      "\n",
      "SVM - rbf Kernel\n",
      "ROC -  0.5\n",
      "Confusion Matrix -  [[  0  14]\n",
      " [  0 200]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.00      0.00      0.00        14\n",
      "    class 1       0.93      1.00      0.97       200\n",
      "\n",
      "avg / total       0.87      0.93      0.90       214\n",
      "\n",
      "Random Forest\n",
      "ROC -  0.539285714286\n",
      "Confusion Matrix -  [[  0  14]\n",
      " [  4 196]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.00      0.00      0.00        14\n",
      "    class 1       0.93      0.98      0.96       200\n",
      "\n",
      "avg / total       0.87      0.92      0.89       214\n",
      "\n",
      "Gaussian NB\n",
      "ROC -  0.482857142857\n",
      "Confusion Matrix -  [[  0  14]\n",
      " [  0 200]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.00      0.00      0.00        14\n",
      "    class 1       0.93      1.00      0.97       200\n",
      "\n",
      "avg / total       0.87      0.93      0.90       214\n",
      "\n",
      "Bernoulli NB\n",
      "ROC -  0.5\n",
      "Confusion Matrix -  [[  0  14]\n",
      " [  0 200]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.00      0.00      0.00        14\n",
      "    class 1       0.93      1.00      0.97       200\n",
      "\n",
      "avg / total       0.87      0.93      0.90       214\n",
      "\n",
      "SVM Linear Kernel\n",
      "ROC -  0.475\n",
      "Confusion Matrix -  [[  6   8]\n",
      " [ 77 123]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.07      0.43      0.12        14\n",
      "    class 1       0.94      0.61      0.74       200\n",
      "\n",
      "avg / total       0.88      0.60      0.70       214\n",
      "\n",
      "Nearest Neighbors\n",
      "ROC -  0.392857142857\n",
      "Confusion Matrix -  [[  0  14]\n",
      " [  0 200]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.00      0.00      0.00        14\n",
      "    class 1       0.93      1.00      0.97       200\n",
      "\n",
      "avg / total       0.87      0.93      0.90       214\n",
      "\n",
      "Logistic Regression\n",
      "ROC -  0.523928571429\n",
      "Confusion Matrix -  [[  0  14]\n",
      " [  0 200]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.00      0.00      0.00        14\n",
      "    class 1       0.93      1.00      0.97       200\n",
      "\n",
      "avg / total       0.87      0.93      0.90       214\n",
      "\n",
      "XG Boost\n",
      "ROC -  0.279642857143\n",
      "Confusion Matrix -  [[  0  14]\n",
      " [  0 200]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.00      0.00      0.00        14\n",
      "    class 1       0.93      1.00      0.97       200\n",
      "\n",
      "avg / total       0.87      0.93      0.90       214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_classifier(x_train, y_train, x_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
