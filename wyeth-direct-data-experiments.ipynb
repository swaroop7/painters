{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swaroop\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "make_submission() generates predictions for the Kaggle Painter by Numbers competion\n",
    "using simple features (image size, aspect ratio and bits/pixel^2)\n",
    "author: Swaroop Krothapalli - extended code of small yello duck\n",
    "https://github.com/swaroop7/painters\n",
    "'''\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cross_validation import KFold\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.stats import itemfreq\n",
    "from sklearn import neighbors, linear_model\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn import cross_validation\n",
    "np.set_printoptions(precision=3, linewidth=100)\n",
    "\n",
    "mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Wyeths')\n",
    "\n",
    "def getEntropy(signal):\n",
    "    lensig=signal.size\n",
    "    symset=list(set(signal))\n",
    "    numsym=len(symset)\n",
    "    probabability_distribution=[np.size(signal[signal==i])/(1.0*lensig) for i in symset]\n",
    "    entropy=np.sum([p*np.log2(1.0/p) for p in probabability_distribution])\n",
    "    return entropy\n",
    "\n",
    "def calculateEntropyNeighbourhood(artwork, neighbourhood):\n",
    "    image = cv2.imread(artwork)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    colorIm=np.array(image)\n",
    "    grayIm=np.array(gray_image)\n",
    "    \n",
    "    N=neighbourhood\n",
    "    S=grayIm.shape\n",
    "    E=np.array(grayIm)\n",
    "    \n",
    "    for row in range(S[0]): \n",
    "            for col in range (S[1]): \n",
    "                    Lx=np.max([0,col-N]) \n",
    "                    Ux=np.min([S[1],col+N])\n",
    "                    Ly=np.max([0,row-N])\n",
    "                    Uy=np.min([S[0],row+N])\n",
    "                    # makes region 1-D\n",
    "                    region=grayIm[Ly:Uy,Lx:Ux].flatten()\n",
    "                    E[row,col]=getEntropy(region)\n",
    "    \n",
    "    average=np.mean(E)\n",
    "    return average\n",
    "\n",
    "def getDTM(artwork, neighbourhood):\n",
    "    image = cv2.imread(artwork)\n",
    "    image32f = np.float32(image)\n",
    "    mu    = cv2.blur(image32f,(5,5))\n",
    "    mu2   = cv2.blur(cv2.multiply(image32f,image32f), (5,5))\n",
    "    sigma = cv2.sqrt( mu2 - cv2.multiply(mu, mu) )\n",
    "    return np.mean(sigma)\n",
    "\n",
    "#image_info_test = get_image_info(test_info, 'test')\n",
    "def get_image_info(test_info, dir):\n",
    "\tif dir == 'test':\n",
    "\t\timages = list(set(list(test_info.image1.unique()) + list(test_info.image2.unique())))\n",
    "\t\tinfo = pd.DataFrame(np.array(images).reshape((-1, 1)), columns = ['filename'])\n",
    "\t\t#print info\n",
    "\telse:\n",
    "\t\tinfo = test_info\n",
    "\t\n",
    "\tinfo['pixelsx'] = np.nan\n",
    "\tinfo['pixelsy'] = np.nan\n",
    "\tinfo['size_bytes'] = np.nan\n",
    "\tinfo['entropy1'] = np.nan\n",
    "\tinfo['entropy5'] = np.nan\n",
    "\tinfo['entropy10'] = np.nan\n",
    "\tinfo['entropy15'] = np.nan\n",
    "# \tinfo['entropy20'] = np.nan\n",
    "#\tinfo['dtm'] = np.nan\n",
    "\tinfo['dtm1'] = np.nan\n",
    "\tinfo['dtm5'] = np.nan\n",
    "\tinfo['dtm10'] = np.nan\n",
    "\tinfo['dtm15'] = np.nan\n",
    "\n",
    "    \n",
    "\t\n",
    "\tj = 0\n",
    "\tfor i in info.index.values:\n",
    "\t\tj += 1        \n",
    "\t\ttry:\n",
    "\t\t\t#print i\n",
    "\t\t\t#fil = 'C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Wyeths\\\\'+dir+'\\\\'+info.loc[i, 'filename']\n",
    "\t\t\tfil = 'C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Wyeths\\\\'+info.loc[i, 'filename']\n",
    "\t\t\t#print fil\n",
    "\t\t\tim = Image.open('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Wyeths\\\\'+info.loc[i, 'filename'])\n",
    "\t\t\t#print im\n",
    "\t\t\t#print im.size\n",
    "\t\t\tinfo.loc[i, 'pixelsx'], info.loc[i, 'pixelsy'] = im.size\n",
    "\t\t\t#im = cv2.imread(dir+'/'+info.loc[i, 'new_filename'])\n",
    "\t\t\t#info.loc[i, 'pixelsx'], info.loc[i, 'pixelsy'] = im.shape[0:2]\n",
    "\t\t\tinfo.loc[i, 'size_bytes'] = os.path.getsize(info.loc[i, 'filename'])\n",
    "\t\t\t#info.loc[i, 'entropy'] = calculateEntropyNeighbourhood(fil, 1)\n",
    "\t\t\t#print calculateEntropyNeighbourhood(fil, 1)\n",
    "\t\t\t#info.loc[i, 'dtm'] = getDTM(fil)\n",
    "\t\t\tinfo.loc[i, 'entropy1'] = calculateEntropyNeighbourhood(fil, 1)\n",
    "\t\t\tinfo.loc[i, 'entropy5'] = calculateEntropyNeighbourhood(fil, 5)\n",
    "\t\t\tinfo.loc[i, 'entropy10'] = calculateEntropyNeighbourhood(fil, 10)\n",
    "\t\t\tinfo.loc[i, 'entropy15'] = calculateEntropyNeighbourhood(fil, 15)\n",
    "# \t\t\tinfo['entropy20'] = calculateEntropyNeighbourhood(fil, 20)\n",
    "\t\t\tinfo.loc[i, 'dtm1'] = getDTM(fil, 1)\n",
    "\t\t\tinfo.loc[i, 'dtm5'] = getDTM(fil, 5)\n",
    "\t\t\tinfo.loc[i, 'dtm10'] = getDTM(fil, 10)\n",
    "\t\t\tinfo.loc[i, 'dtm15'] = getDTM(fil, 15)\n",
    "\n",
    "            \n",
    "            \n",
    "\t\texcept:\n",
    "\t\t\tprint dir+'\\\\'+info.loc[i, 'filename']\n",
    "\t\tif (j%10 == 0):\n",
    "\t\t\tprint '',\n",
    "\tinfo=info.dropna()\n",
    "\tprint 'info shape',info.shape\n",
    "\treturn info.rename(columns={'filename' : 'new_filename'})\n",
    "\n",
    "#t = make_pairs(train_info)\t\n",
    "def make_pairs(train_info):\n",
    "\tprint \"make pairs train info shape\",train_info.shape\n",
    "\tartists = train_info.artist.unique()\n",
    "\n",
    "\tn = train_info.groupby('artist').size()\n",
    "\tn = (2*n**2).sum() \n",
    "\tt = pd.DataFrame(np.zeros((n, 4)), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "\ti = 0\n",
    "\tj = 0\n",
    "\tfor m in artists:\n",
    "\t\t\n",
    "\t\ta = train_info[train_info.artist==m][['artist', 'new_filename']].values\n",
    "\t\tuse = train_info[train_info.artist != m].index.values\n",
    "\t\tprint \"a and use shapes\", a.shape, use.shape\n",
    "\t\tnp.random.shuffle(use)\n",
    "\t\t#print a.shape, use.shape\n",
    "\t\tnm = np.mean([a.shape[0]**2, train_info[train_info.artist != m].shape[0] ])\n",
    "\t\tprint nm\n",
    "\t\tuse = use[0:nm]\n",
    "\t\tprint \"use.shape\",use.shape\n",
    "\t\tb = train_info[train_info.artist!=m][['artist', 'new_filename']].ix[use, :].values\n",
    "\t\t#print nm, use.shape, b.shape\n",
    "\t\ta2 = pd.DataFrame(np.concatenate([np.repeat(a[:, 0], a.shape[0]).reshape((-1,1)), np.repeat(a[:, 1], a.shape[0]).reshape((-1,1)), np.tile(a, (a.shape[0], 1))], axis=1), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "\t\ta2 = a2.loc[0:nm, :]\n",
    "\t\tb2 = pd.DataFrame(np.concatenate([np.tile(a, (a.shape[0], 1))[0:b.shape[0], :], b], axis=1), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "\t\tprint j, i, a2.shape[0], b2.shape[0]\n",
    "\t\t#print b2\n",
    "\t\tt.iloc[i:i+a2.shape[0], :] = a2.values\n",
    "\t\tt.iloc[i+a2.shape[0]:i+a2.shape[0]+b2.shape[0], :] = b2.values\n",
    "\t\ti += a2.shape[0] +b2.shape[0]\n",
    "\t\tj += 1\n",
    "\t\n",
    "\tt = t[~t.image2.isin([np.nan, 0])]\n",
    "\tprint t.shape, t[t.image1 > t.image2].shape\n",
    "\tprint t.columns.values\n",
    "\t#print t\n",
    "\tprint \"hi1\",t.drop_duplicates(subset=['artist1', 'artist2','image1', 'image2'], keep=False).shape\n",
    "\t#return t[t.image1 > t.image2]\t\n",
    "\treturn t.drop_duplicates(subset=['artist1', 'artist2','image1', 'image2'], keep=False)\n",
    "\n",
    "\n",
    "#x_train, y_train, x_cv, y_cv = prep_data([train_info, None], 'cv')\t\n",
    "#x_test, y_test = prep_data([None, submission_info], 'test')\t\n",
    "def prep_data(input, split):\n",
    "\tinfo = input[0]\n",
    "\tdata = input[1]\n",
    "\t\n",
    "\tif split=='cv':\n",
    "\t\t#artists = info.artist.unique()\n",
    "\t\tartists = info.artist\n",
    "\t\t#print artists\n",
    "\t\t#print 'hi', artists\n",
    "\t\tnp.random.shuffle(artists)\n",
    "\t\t\n",
    "\t\tinfo = get_image_info(info, 'train')\n",
    "\t\tinfo['bytes_per_pixel'] = 1.0*info['size_bytes']/(info['pixelsx']*info['pixelsy'])\n",
    "\t\tinfo['aspect_ratio'] = 1.0*info['pixelsx']/info['pixelsy']\n",
    "\t\t#train_artists = artists[0:int(0.8*len(artists))]\n",
    "\t\t#test_artists = artists[int(0.8*len(artists)):]\n",
    "\t\t#print artists\n",
    "\t\t#print 'hi',info[info.artist.isin(artists)].shape\n",
    "\t\t#train = make_pairs(info[info.artist.isin(artists)])\n",
    "\t\t#test = make_pairs(info[info.artist.isin(test_artists)])\n",
    "\t\t#print train.shape\n",
    "\t\t#train['in_train'] = True\n",
    "\t\t#test['in_train'] = True\n",
    "\t\t#data = train\n",
    "\t\t#data['sameArtist'] = data['artist1'] == data['artist2']\n",
    "\t\tprint info.columns\n",
    "\n",
    "        \n",
    "\tif split=='test':\n",
    "\n",
    "\t\tinfo = get_image_info(data, 'test')\n",
    "\t\tinfo['bytes_per_pixel'] = 1.0*info['size_bytes']/(info['pixelsx']*info['pixelsy'])\n",
    "\t\tinfo['aspect_ratio'] = 1.0*info['pixelsx']/info['pixelsy']\t\n",
    "\t\t\n",
    "\t\tdata['in_train'] = False\n",
    "\t\n",
    "\t\tif 'artist1' in data.columns:\n",
    "\t\t\tdata['sameArtist'] = data['artist1'] == data['artist2']\n",
    "\n",
    "\n",
    "\tinfo['artist'] = info['artist'].map({'watercolor': 1, 'tempura': 0})\n",
    "\ty_train = info['artist']\n",
    "\tinfo = info.drop(['Name', 'new_filename', 'year', 'artist', 'artist_orig'], axis=1)\n",
    "\tx_train = info\n",
    "# \tdata2 = pd.merge(data, info[['new_filename', 'pixelsx', 'pixelsy', 'size_bytes', 'bytes_per_pixel', 'aspect_ratio']], how='left', left_on='image1', right_on='new_filename')\n",
    "# \tdata2.drop('new_filename', 1, inplace=True)\n",
    "\t\n",
    "# \tdata2 = pd.merge(data2, info[['new_filename', 'pixelsx', 'pixelsy', 'size_bytes', 'bytes_per_pixel', 'aspect_ratio']], how='left', left_on='image2', right_on='new_filename')\n",
    "# \tdata2.drop('new_filename', 1, inplace=True)\n",
    "\t\n",
    "# \tx_train = data2[data2.in_train==True][['pixelsx_x', 'pixelsy_x', 'size_bytes_x', 'bytes_per_pixel_x', 'aspect_ratio_x', 'pixelsx_y', 'pixelsy_y', 'size_bytes_y', 'bytes_per_pixel_y', 'aspect_ratio_y']].values\n",
    "# \tx_test = data2[data2.in_train==False][['pixelsx_x', 'pixelsy_x', 'size_bytes_x', 'bytes_per_pixel_x', 'aspect_ratio_x', 'pixelsx_y', 'pixelsy_y', 'size_bytes_y', 'bytes_per_pixel_y', 'aspect_ratio_y']].values\n",
    "\t\n",
    "\t\n",
    "# \tif 'artist1' in data.columns: \n",
    "# \t\ty_train = data2[data2.in_train==True]['sameArtist'].values\n",
    "# \t\ty_test = data2[data2.in_train==False]['sameArtist'].values\n",
    "# \telse:\n",
    "# \t\ty_test = None\t\n",
    "\t\n",
    "\tif split=='cv':\t\t\n",
    "\t\treturn x_train, y_train, x_train, y_train  \n",
    " \tif split=='test':\n",
    "\t\treturn x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepping training and cv data\n",
      "train\\Above the Orchard-1957.jpg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "train_info = pd.read_csv('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\wyeths_medium.csv')\n",
    "#submission_info = pd.read_csv('submission_info.csv')\n",
    "print 'prepping training and cv data'\n",
    "x_train, y_train, x_cv, y_cv = prep_data([train_info, None], 'cv')\n",
    "\n",
    "print x_train.shape\n",
    "\n",
    "#np.savetxt('x_train_wyeth.txt', x_train, fmt = '%1.3f' )\n",
    "#np.savetxt('y_train_wyeth.txt', y_train, fmt = '%1.3f' )\n",
    "\n",
    "print (time.time() - start_time)/60 , \"minutes\"\n",
    "\n",
    "#print 'prepping test data'\n",
    "#x_test, y_test = prep_data([None, submission_info], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swaroop\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f68765f77dbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ent5_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy5'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ent10_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy10'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ent15_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy15'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ent10_5'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy10'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy5'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ent15_5'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy15'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy5'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "x_train['ent5_1'] = x_train['entropy5'] - x_train['entropy1']\n",
    "x_train['ent10_1'] = x_train['entropy10'] - x_train['entropy1']\n",
    "x_train['ent15_1'] = x_train['entropy15'] - x_train['entropy1']\n",
    "x_train['ent10_5'] = x_train['entropy10'] - x_train['entropy5']\n",
    "x_train['ent15_5'] = x_train['entropy15'] - x_train['entropy5']\n",
    "x_train['ent15_10'] = x_train['entropy15'] - x_train['entropy10']\n",
    "\n",
    "\n",
    "x_train['ent5d1'] = x_train['entropy5'] / x_train['entropy1']\n",
    "x_train['ent10d1'] = x_train['entropy10'] / x_train['entropy1']\n",
    "x_train['ent15d1'] = x_train['entropy15'] / x_train['entropy1']\n",
    "x_train['ent10d5'] = x_train['entropy10'] / x_train['entropy5']\n",
    "x_train['ent15d5'] = x_train['entropy15'] / x_train['entropy5']\n",
    "x_train['ent15d10'] = x_train['entropy15'] / x_train['entropy10']\n",
    "\n",
    "\n",
    "x_train['ent1d5'] = x_train['entropy1'] / x_train['entropy5']\n",
    "x_train['ent1d10'] = x_train['entropy1'] / x_train['entropy10']\n",
    "x_train['ent1d15'] = x_train['entropy1'] / x_train['entropy15']\n",
    "x_train['ent5d10'] = x_train['entropy5'] / x_train['entropy10']\n",
    "x_train['ent5d15'] = x_train['entropy5'] / x_train['entropy15']\n",
    "x_train['ent10d15'] = x_train['entropy10'] / x_train['entropy15']\n",
    "\n",
    "x_train['dtm10_5'] = x_train['dtm10'] - x_train['dtm5']\n",
    "x_train['dtm15_5'] = x_train['dtm15'] - x_train['dtm5']\n",
    "x_train['dtm15_10'] = x_train['dtm15'] - x_train['dtm10']\n",
    "\n",
    "x_train['dtm10d5'] = x_train['dtm10'] / x_train['dtm5']\n",
    "x_train['dtm15d5'] = x_train['dtm15'] / x_train['dtm5']\n",
    "x_train['dtm15d10'] = x_train['dtm15'] / x_train['dtm10']\n",
    "\n",
    "x_train['dtm5d10'] = x_train['dtm5'] / x_train['dtm10']\n",
    "x_train['dtm5d15'] = x_train['dtm5'] / x_train['dtm15']\n",
    "x_train['dtm10d15'] = x_train['dtm10'] / x_train['dtm15']\n",
    "\n",
    "def print_results(clf, y_test, y_pred, y_pred_prob):\n",
    "    #y_pred_prob = clf.predict_proba(y_test)[:,1]\n",
    "    #y_pred = clf.predict(y_test)\n",
    "    print 'ROC - ',roc_auc_score(y_test, y_pred_prob)\n",
    "    print 'Accuracy - ',accuracy_score(y_test, y_pred)\n",
    "    print 'Confusion Matrix - ', confusion_matrix(y_test, y_pred)\n",
    "    #print 'Precision - ',precision_score(y_test, y_pred ),'Recall - ',recall_score(y_test, y_pred),'F1- Score',f1_score(y_test, y_pred),'\\n'\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    print classification_report(y_test, y_pred, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = np.loadtxt('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Wyeths\\\\x_train_wyeth.txt')\n",
    "y_train = np.loadtxt('C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Wyeths\\\\y_train_wyeth.txt')\n",
    "\n",
    "def print_results(clf, y_test, y_pred, y_pred_prob):\n",
    "    #y_pred_prob = clf.predict_proba(y_test)[:,1]\n",
    "    #y_pred = clf.predict(y_test)\n",
    "    print 'ROC - ',roc_auc_score(y_test, y_pred_prob)\n",
    "    print 'Accuracy - ',accuracy_score(y_test, y_pred)\n",
    "    print 'Confusion Matrix - ', confusion_matrix(y_test, y_pred)\n",
    "    #print 'Precision - ',precision_score(y_test, y_pred ),'Recall - ',recall_score(y_test, y_pred),'F1- Score',f1_score(y_test, y_pred),'\\n'\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    print classification_report(y_test, y_pred, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50L, 28L) (50L,)\n",
      "(40L, 28L) (10L, 28L)\n",
      "[[  0.  22.]\n",
      " [  1.  18.]]\n",
      "[[ 0.  5.]\n",
      " [ 1.  5.]]\n",
      "[[ 0.  4.]\n",
      " [ 1.  4.]]\n",
      "[[  0.  27.]\n",
      " [  1.  23.]]\n",
      "[ 0.  0.  1.  1.  0.  1.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape, y_train.shape\n",
    "training, testing, y_training, y_testing = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "actual_training, validation, y_actual_training, y_validation = train_test_split(training, y_training, test_size=0.2, stratify=y_training, random_state=42)\n",
    "print training.shape, testing.shape\n",
    "#print y_training, y_testing\n",
    "print itemfreq(y_training)\n",
    "print itemfreq(y_testing)\n",
    "print itemfreq(y_validation)\n",
    "print itemfreq(y_train)\n",
    "\n",
    "print y_validation\n",
    "# print y_actual_training\n",
    "# print actual_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY RESUTLS\n",
      "Mean Accuracy of 5 fold CV \tSTD \tValidation  \tTest\n",
      "0 0.549523809524 0.202066422253 0.625 0.3\n",
      "ROC -  0.18\n",
      "Accuracy -  0.3\n",
      "Confusion Matrix -  [[2 3]\n",
      " [4 1]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.33      0.40      0.36         5\n",
      "    class 1       0.25      0.20      0.22         5\n",
      "\n",
      "avg / total       0.29      0.30      0.29        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "print \"ACCURACY RESUTLS\"    \n",
    "print \"Mean Accuracy of 5 fold CV\",\"\\t\", \"STD\",\"\\t\", \"Validation \",\"\\t\", \"Test\"        \n",
    "\n",
    "for i in xrange(1):\n",
    "    clf = RandomForestClassifier(n_estimators=20, class_weight='balanced', n_jobs = -1, random_state = 42)\n",
    "    clf.fit(actual_training, y_actual_training)\n",
    "    scores = cross_validation.cross_val_score(clf, actual_training, y_actual_training, cv=5, n_jobs = -1)\n",
    "    y_pred_valid = clf.predict(validation)\n",
    "    print i, scores.mean(), scores.std(), accuracy_score(y_validation, y_pred_valid), accuracy_score(y_testing, clf.predict(testing))\n",
    "    \n",
    "print_results(clf, y_testing, clf.predict(testing), clf.predict_proba(testing)[:,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY RESUTLS\n",
      "Mean Accuracy of 5 fold CV \tSTD \tValidation  \tTest\n",
      "0 \t0.359047619048 \t0.177705200605 \t0.5 \t0.6\n",
      "ROC -  0.32\n",
      "Accuracy -  0.6\n",
      "Confusion Matrix -  [[4 1]\n",
      " [3 2]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.57      0.80      0.67         5\n",
      "    class 1       0.67      0.40      0.50         5\n",
      "\n",
      "avg / total       0.62      0.60      0.58        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#XGB\n",
    "    \n",
    "learning_r_col = [0.01, 0.02, 0.03, 0.04,0.05,0.06,0.07,0.08,0.09,0.1]    \n",
    "    \n",
    "print \"ACCURACY RESUTLS\"    \n",
    "print \"Mean Accuracy of 5 fold CV\",\"\\t\", \"STD\",\"\\t\", \"Validation \",\"\\t\", \"Test\"        \n",
    "\n",
    "    \n",
    "for i in xrange(1):\n",
    "    clf = xgb.XGBClassifier(max_depth=2, n_estimators=48, learning_rate=0.05, nthread = -1) #objective='multi:softprob'\n",
    "    clf.fit(actual_training, y_actual_training)\n",
    "    scores = cross_validation.cross_val_score(clf, actual_training, y_actual_training, cv=5, n_jobs = -1)\n",
    "    y_pred_valid = clf.predict(validation)\n",
    "    print i, \"\\t\",scores.mean(),\"\\t\", scores.std(), \"\\t\",accuracy_score(y_validation, y_pred_valid), \"\\t\",accuracy_score(y_testing, clf.predict(testing))\n",
    "\n",
    "print_results(clf, y_testing, clf.predict(testing), clf.predict_proba(testing)[:,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY RESUTLS\n",
      "Mean Accuracy of 5 fold CV \tSTD \tValidation  \tTest\n",
      "0 \t0.606666666667 \t0.237477293341 \t0.375 \t0.4\n",
      "ROC -  0.16\n",
      "Accuracy -  0.4\n",
      "Confusion Matrix -  [[3 2]\n",
      " [4 1]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.43      0.60      0.50         5\n",
      "    class 1       0.33      0.20      0.25         5\n",
      "\n",
      "avg / total       0.38      0.40      0.38        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic\n",
    "    \n",
    "print \"ACCURACY RESUTLS\"    \n",
    "print \"Mean Accuracy of 5 fold CV\",\"\\t\", \"STD\",\"\\t\", \"Validation \",\"\\t\", \"Test\"    \n",
    "    \n",
    "for i in xrange(1):  # , max_iter = i\n",
    "    clf = linear_model.LogisticRegression( class_weight='balanced')\n",
    "    clf.fit(actual_training, y_actual_training)\n",
    "    scores = cross_validation.cross_val_score(clf, actual_training, y_actual_training, cv=5, n_jobs = -1)\n",
    "    y_pred_valid = clf.predict(validation)\n",
    "    print i, \"\\t\",scores.mean(),\"\\t\", scores.std(), \"\\t\",accuracy_score(y_validation, y_pred_valid), \"\\t\",accuracy_score(y_testing, clf.predict(testing))\n",
    "    \n",
    "print_results(clf, y_testing, clf.predict(testing), clf.predict_proba(testing)[:,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY RESUTLS of 5 fold CV\n",
      "Mean Accuracy  \tSTD \tValidation  \tTest\n",
      "10 \t0.618095238095 \t0.14662955622 \t0.375 \t0.4\n",
      "100 \t0.578095238095 \t0.212950336475 \t0.375 \t0.4\n",
      "500 \t0.578095238095 \t0.212950336475 \t0.375 \t0.4\n",
      "1000 \t0.578095238095 \t0.212950336475 \t0.375 \t0.4\n",
      "Confusion Matrix -  [[3 2]\n",
      " [4 1]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.43      0.60      0.50         5\n",
      "    class 1       0.33      0.20      0.25         5\n",
      "\n",
      "avg / total       0.38      0.40      0.38        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Linear SVC\n",
    "    \n",
    "print \"ACCURACY RESUTLS of 5 fold CV\"    \n",
    "print \"Mean Accuracy \",\"\\t\", \"STD\",\"\\t\", \"Validation \",\"\\t\", \"Test\"    \n",
    "\n",
    "iter = [10,100,500,1000]\n",
    "\n",
    "for i in iter:\n",
    "    clf = LinearSVC(class_weight = 'balanced', dual = False, max_iter = i)\n",
    "    clf.fit(actual_training, y_actual_training)\n",
    "    scores = cross_validation.cross_val_score(clf, actual_training, y_actual_training, cv=5, n_jobs = -1)\n",
    "    y_pred_valid = clf.predict(validation)\n",
    "    print i, \"\\t\",scores.mean(),\"\\t\", scores.std(), \"\\t\",accuracy_score(y_validation, y_pred_valid), \"\\t\",accuracy_score(y_testing, clf.predict(testing))\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print 'Confusion Matrix - ', confusion_matrix(y_testing, clf.predict(testing))\n",
    "print classification_report(y_testing, clf.predict(testing), target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def train_classifier(x_train, y_train, x_cv, y_cv):    \n",
    "    clf1 = SVC(kernel = 'sigmoid',probability=True, tol = 0.01)\n",
    "    clf2 = SVC(kernel = 'rbf',  probability=True, tol = 0.01)\n",
    "    clf3 = RandomForestClassifier(n_estimators=20,class_weight='balanced', n_jobs = -1)\n",
    "    clf4 = GaussianNB()\n",
    "    clf5 = BernoulliNB() \n",
    "    clf6 = LinearSVC(class_weight='balanced',dual = False)\n",
    "    clf7 = SVC(kernel = 'poly', probability=True, tol = 0.1, degree=2, C=1.0)\n",
    "    clf8 = neighbors.KNeighborsClassifier()\n",
    "    clf9 = linear_model.LogisticRegression(class_weight = 'balanced')\n",
    "    clf10 = xgb.XGBClassifier(max_depth=3, n_estimators=30, learning_rate=0.05) #objective='multi:softprob'\n",
    "    print 'starting fit'\n",
    "    \n",
    "    print x_train.shape, y_train.shape\n",
    "    training, testing, y_training, y_testing = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "    \n",
    "    print training.shape, testing.shape\n",
    "    #print y_training, y_testing\n",
    "    print itemfreq(y_training)\n",
    "    print itemfreq(y_testing)\n",
    "    \n",
    "    clf1.fit(training, y_training)\n",
    "#     scores = cross_validation.cross_val_score(clf1, training, y_training, cv=5, scoring='roc_auc', n_jobs = -1)\n",
    "#     print \"SVM - Sigmoid Scores\",\n",
    "#     print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#     clf2.fit(training, y_training)\n",
    "#     scores = cross_validation.cross_val_score(clf2, training, y_training, cv=5, scoring='roc_auc', n_jobs = -1)\n",
    "#     print \"SVM - RBF Kernel\", \n",
    "#     print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    clf3.fit(training, y_training)\n",
    "    scores = cross_validation.cross_val_score(clf3, training, y_training, cv=5,  n_jobs = -1)\n",
    "    print \"Random Forest\", \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#     clf4.fit(training, y_training)\n",
    "#     scores = cross_validation.cross_val_score(clf4, training, y_training, cv=5, scoring='roc_auc', n_jobs = -1)\n",
    "#     print \"Gaussian NB\", \n",
    "#     print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#     clf5.fit(training, y_training) \n",
    "#     scores = cross_validation.cross_val_score(clf5, training, y_training, cv=5, scoring='roc_auc', n_jobs = -1)\n",
    "#     print \"Bernoulli NB\", \n",
    "#     print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    clf6.fit(training, y_training)\n",
    "    scores = cross_validation.cross_val_score(clf6, training, y_training, cv=5,  n_jobs = -1)\n",
    "    print \"SVM - Linear Kernel\", \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#     clf7.fit(training, y_training) \n",
    "#     clf8.fit(training, y_training) \n",
    "#     scores = cross_validation.cross_val_score(clf8, training, y_training, cv=5, scoring='roc_auc', n_jobs = -1)\n",
    "#     print \"KNN\", \n",
    "#     print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    clf9.fit(training, y_training) \n",
    "    scores = cross_validation.cross_val_score(clf9, training, y_training, cv=5,  n_jobs = -1)\n",
    "    print \"Logistic\", \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    clf10.fit(training, y_training)\n",
    "    scores = cross_validation.cross_val_score(clf10, training, y_training, cv=5,  n_jobs = -1)\n",
    "    print \"XGBoost\", \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#     print 'SVM - Sigmoid Kernel'\n",
    "#     print_results(clf1, y_testing, clf1.predict(testing), clf1.predict_proba(testing)[:,1] )\n",
    "#     print 'SVM - rbf Kernel'\n",
    "#     print_results(clf2, y_testing, clf2.predict(testing), clf2.predict_proba(testing)[:,1] )\n",
    "    print 'Random Forest'\n",
    "    print_results(clf3, y_testing, clf3.predict(testing), clf3.predict_proba(testing)[:,1] )\n",
    "#     print 'Gaussian NB'\n",
    "#     print_results(clf4, y_testing, clf4.predict(testing), clf4.predict_proba(testing)[:,1] )\n",
    "#     print 'Bernoulli NB'\n",
    "#     print_results(clf5, y_testing, clf5.predict(testing), clf5.predict_proba(testing)[:,1] )\n",
    "    print 'SVM Linear Kernel'    \n",
    "    prob_pos = clf6.decision_function(testing)\n",
    "    prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "    print_results(clf6, y_testing, clf6.predict(testing), prob_pos )\n",
    "#     print 'SVM Polynomial Kernel'\n",
    "#     print_results(clf7, y_testing, clf7.predict(testing), clf7.predict_proba(testing)[:,1] )\n",
    "#     print 'Nearest Neighbors'\n",
    "#     print_results(clf8, y_testing, clf8.predict(testing), clf8.predict_proba(testing)[:,1] )\n",
    "    print 'Logistic Regression'\n",
    "    print_results(clf9, y_testing, clf9.predict(testing), clf9.predict_proba(testing)[:,1] )\n",
    "    print 'XG Boost'\n",
    "    print_results(clf10, y_testing, clf10.predict(testing), clf10.predict_proba(testing)[:,1] )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting fit\n",
      "(32L, 28L) (32L,)\n",
      "(25L, 28L) (7L, 28L)\n",
      "[[  0.  14.]\n",
      " [  1.  11.]]\n",
      "[[ 0.  4.]\n",
      " [ 1.  3.]]\n",
      "Random Forest Accuracy: 0.36 (+/- 0.39)\n",
      "SVM - Linear Kernel Accuracy: 0.51 (+/- 0.34)\n",
      "Logistic Accuracy: 0.47 (+/- 0.42)\n",
      "XGBoost Accuracy: 0.46 (+/- 0.30)\n",
      "Random Forest\n",
      "ROC -  0.416666666667\n",
      "Accuracy -  0.428571428571\n",
      "Confusion Matrix -  [[2 2]\n",
      " [2 1]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.50      0.50      0.50         4\n",
      "    class 1       0.33      0.33      0.33         3\n",
      "\n",
      "avg / total       0.43      0.43      0.43         7\n",
      "\n",
      "SVM Linear Kernel\n",
      "ROC -  1.0\n",
      "Accuracy -  0.857142857143\n",
      "Confusion Matrix -  [[4 0]\n",
      " [1 2]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.80      1.00      0.89         4\n",
      "    class 1       1.00      0.67      0.80         3\n",
      "\n",
      "avg / total       0.89      0.86      0.85         7\n",
      "\n",
      "Logistic Regression\n",
      "ROC -  1.0\n",
      "Accuracy -  0.857142857143\n",
      "Confusion Matrix -  [[4 0]\n",
      " [1 2]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.80      1.00      0.89         4\n",
      "    class 1       1.00      0.67      0.80         3\n",
      "\n",
      "avg / total       0.89      0.86      0.85         7\n",
      "\n",
      "XG Boost\n",
      "ROC -  0.5\n",
      "Accuracy -  0.571428571429\n",
      "Confusion Matrix -  [[2 2]\n",
      " [1 2]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.67      0.50      0.57         4\n",
      "    class 1       0.50      0.67      0.57         3\n",
      "\n",
      "avg / total       0.60      0.57      0.57         7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_classifier(actual_training, y_actual_training, validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
